{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44be304d",
   "metadata": {},
   "source": [
    "## Li's Machine Learning GPT Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdcb8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can uncomment the following lines to download the raw text file \"input.txt\"\n",
    "# But that file is already provided in the git repo\n",
    "\n",
    "# import urllib.request\n",
    "# urllib.request.urlretrieve(\n",
    "#     \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\", \n",
    "#     \"input.txt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8bf03cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296ec549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8df696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1884bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80270935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b767dca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639e858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896c5f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9898d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24aba988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8bab4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77619aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a35edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0471adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.587916374206543\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6da7aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
      "3Q&sGlvHQ?mqSq-eON\n",
      "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
      "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
      "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
      "&WDdP!Ko,px\n",
      "x\n",
      "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
      "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067795a",
   "metadata": {},
   "source": [
    "# The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a1860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f043e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c35ecf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05d75b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aef6884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8dfb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db4e9e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177f48f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0b9a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb9e2d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2705b105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50b0624a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0de30326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abe33eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd3b65ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "894b20c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b44b961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.5763e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1da9e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0b02b",
   "metadata": {},
   "source": [
    "## Full finished code, for reference\n",
    "\n",
    "### You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c4ae227",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4197, val loss 2.4338\n",
      "step 400: train loss 2.3504, val loss 2.3567\n",
      "step 500: train loss 2.2966, val loss 2.3131\n",
      "step 600: train loss 2.2410, val loss 2.2502\n",
      "step 700: train loss 2.2053, val loss 2.2188\n",
      "step 800: train loss 2.1639, val loss 2.1875\n",
      "step 900: train loss 2.1243, val loss 2.1503\n",
      "step 1000: train loss 2.1032, val loss 2.1305\n",
      "step 1100: train loss 2.0694, val loss 2.1186\n",
      "step 1200: train loss 2.0378, val loss 2.0793\n",
      "step 1300: train loss 2.0249, val loss 2.0627\n",
      "step 1400: train loss 1.9937, val loss 2.0373\n",
      "step 1500: train loss 1.9713, val loss 2.0309\n",
      "step 1600: train loss 1.9636, val loss 2.0487\n",
      "step 1700: train loss 1.9429, val loss 2.0160\n",
      "step 1800: train loss 1.9098, val loss 1.9962\n",
      "step 1900: train loss 1.9095, val loss 1.9882\n",
      "step 2000: train loss 1.8859, val loss 1.9972\n",
      "step 2100: train loss 1.8714, val loss 1.9762\n",
      "step 2200: train loss 1.8594, val loss 1.9612\n",
      "step 2300: train loss 1.8547, val loss 1.9518\n",
      "step 2400: train loss 1.8429, val loss 1.9462\n",
      "step 2500: train loss 1.8161, val loss 1.9439\n",
      "step 2600: train loss 1.8277, val loss 1.9392\n",
      "step 2700: train loss 1.8122, val loss 1.9325\n",
      "step 2800: train loss 1.8045, val loss 1.9224\n",
      "step 2900: train loss 1.8008, val loss 1.9273\n",
      "step 3000: train loss 1.7990, val loss 1.9238\n",
      "step 3100: train loss 1.7694, val loss 1.9162\n",
      "step 3200: train loss 1.7533, val loss 1.9134\n",
      "step 3300: train loss 1.7588, val loss 1.9089\n",
      "step 3400: train loss 1.7546, val loss 1.8951\n",
      "step 3500: train loss 1.7367, val loss 1.8917\n",
      "step 3600: train loss 1.7240, val loss 1.8871\n",
      "step 3700: train loss 1.7285, val loss 1.8830\n",
      "step 3800: train loss 1.7180, val loss 1.8899\n",
      "step 3900: train loss 1.7242, val loss 1.8740\n",
      "step 4000: train loss 1.7118, val loss 1.8555\n",
      "step 4100: train loss 1.7140, val loss 1.8739\n",
      "step 4200: train loss 1.7049, val loss 1.8623\n",
      "step 4300: train loss 1.6999, val loss 1.8504\n",
      "step 4400: train loss 1.7077, val loss 1.8680\n",
      "step 4500: train loss 1.6890, val loss 1.8525\n",
      "step 4600: train loss 1.6882, val loss 1.8375\n",
      "step 4700: train loss 1.6860, val loss 1.8491\n",
      "step 4800: train loss 1.6667, val loss 1.8448\n",
      "step 4900: train loss 1.6684, val loss 1.8380\n",
      "step 4999: train loss 1.6627, val loss 1.8235\n",
      "\n",
      "ROMEO:\n",
      "But you far you lovess; will thus;\n",
      "Weter now I uDWARD IV:\n",
      "It not have you\n",
      "Your his the mustiens, and say wre our his calind, litgle.\n",
      "\n",
      "ANTOLYCUCENTIO:\n",
      "Where hanger to sleun thus mindstress;\n",
      "But, for thom tow, good it norius\n",
      "What his sine; there some nother, sI a hird,----\n",
      "But thou sging them but\n",
      "am they beens more o' old wis; I\n",
      "have down bide pits.\n",
      "\n",
      "KING RICHARD II:\n",
      "Why, by, better, you, was mait?\n",
      "\n",
      "POMPEREY:\n",
      "By favather:\n",
      "But, the hoese and little sweet that,\n",
      "Any not will greive his god and eniring:\n",
      "With Most the ways! doth as father staid us,\n",
      "As some passess ston the earth, arled,\n",
      "Deen some to and potaty's me wold so peat him\n",
      "is fortunuel made them you weat you\n",
      "Why dearss.\n",
      "\n",
      "PORlENBY:\n",
      "Romes!\n",
      "Lord:\n",
      "Well-mone Herry drepity, let us!\n",
      "He a Recrompring it my priceling\n",
      "What colves mote imment Wear myself!\n",
      "\n",
      "Secury, pet with a peased:\n",
      "Hus a croome!' these warly; man.\n",
      "\n",
      "BRUTUS:\n",
      "Well bear confargive, and sou, you, profort.\n",
      "\n",
      "\n",
      "GREMIO:\n",
      "What, whose mistrest,\n",
      "Then fatel be thesee why with. I bein'd?\n",
      "\n",
      "ISABELLA:\n",
      "It we was but to good it tome would patch evile,\n",
      "And that warly that any, I usquesch\n",
      "Frivess villing down the I-woll, and well stopprow--Plaint.\n",
      "Must thou worse is senst my connor undolien. But, by mystless\n",
      "Sour I his cranst, now Mery mustater, though who speak\n",
      "Your bout than me that too the wishalte,\n",
      "wilten the briyy and, on your, fords frul's some.\n",
      "Heed like to be am\n",
      "in him, and I his it no arscred!\n",
      "My lay whose not sun, the\n",
      "wen you will rest: if brive, here leved and madie.\n",
      "\n",
      "PONTER:\n",
      "O, town, up.\n",
      "\n",
      "CADWID' God their pit;\n",
      "What you hearts neithin aftelen.\n",
      "\n",
      "QUEEN MIontener:\n",
      "All even nett, know make-banity;\n",
      "Terst a lameriant that brird othray'st.\n",
      "\n",
      "Sirnesrer:\n",
      "Yet Is o' be the moth,\n",
      "First, I it have.\n",
      "\n",
      "Setherefore!\n",
      "For Lence thmy broughtle the. Go figh unfeed that with then,\n",
      "Thus whilowing thoust we do, is all more.\n",
      "\n",
      "PRIVENTIUS:\n",
      "Yet, York these that high pray plattarn bring not briads mustings weyen in clyself too to requing on prant what you grace in\n",
      "Conguely that withought to\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e9d25c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And thore beless, your awhile will grant grave to be:\n",
      "Cerir with dighmner this quone, I but his good.\n",
      "\n",
      "RICHAMIUS:\n",
      "Is canse of die tonder the the day:\n",
      "Bamy malter-dry me yough: they Hustash tary, not but I cannot, to\n",
      "Thou anttings thee,\n",
      "Where aster, appedied Deaught, by them: this speettery if not you not,\n",
      "Henter, fawit rew. his good me the my paurful?\n",
      "\n",
      "ROMIO:\n",
      "Most. May, maker Mething sut, yet:\n",
      "A trungled, staids mistray mount twish they graves. Yon.\n",
      "\n",
      "Clquservant:\n",
      "Is gate?\n",
      "Noy, now you read own.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "But's thou broth, you, my live,\n",
      "As madely no God-seets is till cosming:\n",
      "Our rovegue?\n",
      "I would loves minost.\n",
      "\n",
      "BOWNORY:\n",
      "In shoman:\n",
      "Then, acvouse,\n",
      "Calfament, not will may that I have\n",
      "Feetens to besut with with to was fall;\n",
      "I'lll, I by my many fivend; eyey his anside to her dead,\n",
      "For as you, arry must I say aggainst.\n",
      "\n",
      "RigLAND:\n",
      "Ay, I have it,\n",
      "That you banish scervant that quarry the latcainstablemed\n",
      "God stild thou lost with all you beath;\n",
      "And while griant fortusent that ded?\n",
      "\n",
      "ROMEO:\n",
      "The devoir litts my like by it: and thus\n",
      "for notitbre,\n",
      "With he stuchild to have I miskners aumost troath.-\n",
      "On, and soets me: atter heard may, ever with woes!\n",
      "Thraint the cold lusior lunges, worth,\n",
      "And the brother sweet wishre hand be as mone.\n",
      "\n",
      "FARINIUS:\n",
      "Thou sive what I have you longs,\n",
      "Say, bray, from though; not with is most.\n",
      "\n",
      "JOLINA:\n",
      "That it where that the by those that her\n",
      "usly father that will'd all you clet Rigntruct,\n",
      "Holy mistry's sout\n",
      "To once myser fliall her, then,\n",
      "The were of you with parth him\n",
      "To come, but rulicious? new are in worse ktweep is day is artuel his dentry your\n",
      "gravish that you wills innot a weep's art?\n",
      "What they the have a thus, with\n",
      "the what my urse art an passely life;\n",
      "I will 'Fgreess her sleive him mind ensby, markin i' gome the iress\n",
      "To talland ew, my old heave in thon acgradle not\n",
      "doefains stwer mily-dread\n",
      "Stay.\n",
      "My lock'd she's them lords\n",
      "not we say's genently bear us recry to Istand fair\n",
      "Mushing liestlest nenget theirs no best with them.\n",
      "\n",
      "Seecire it put the \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b988a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A peach mind, they lovest the my good\n",
      "Tutyn munned upon up her\n",
      "My my behot prild that's lost is than deeper,\n",
      "And blood but there crave this pretter of\n",
      "Your heave tradl up? why what scappe\n",
      "a be-quence to our king. Gives'd be, us you shap\n",
      "He 'seaved thou impasage too, matter us my honer. Mond-fater, we outy.\n",
      "\n",
      "PAULINA:\n",
      "RightABELLA:\n",
      "You both you seakd you hithrid on beefored mudd,\n",
      "slavild. whose know sbeak make if men\n",
      "Thuse basster and highers.\n",
      "\n",
      "PUCKING HENRY VI:\n",
      "Day! where still dows and it you have lant frient, Jentle at; thou king to now.\n",
      "\n",
      "POR CARmNGE:\n",
      "Though, crecting me. I' am thre weepal:\n",
      "I not shoad in tialest entuse other\n",
      "Beaut brotk, you exar wars, ands I know weet; my live\n",
      "MaNENter.\n",
      "\n",
      "HORTRINIO:\n",
      "I hear they you dow then seal dom thou deep the town on;\n",
      "And whritims, that you be Poldity fleith then,\n",
      "Would looks, hene in nim made whilsbattion: enaish like. Give taces.\n",
      "\n",
      "ROMIOLIOL:\n",
      "Priveoost Angelo, thy\n",
      "not bawds wordering us; when, deseir,\n",
      "Om, for the long not pratter aboug;\n",
      "And boy and reachs at is syaing,\n",
      "ANG the grlows have with The the banite,\n",
      "Gost be to or any purly to the pery\n",
      "Of Henrights foldst let not mity toistracious.\n",
      "\n",
      "ANGENIUS:\n",
      "Our cancations:\n",
      "You of bay, he murner'd\n",
      "Mareing so, set strumysted, with tearthder in't.\n",
      "\n",
      "And, what andsbant:\n",
      "That thee, yiell I must to body your;\n",
      "I thou have is un my say; and that the justant.\n",
      "\n",
      "BRUTUS:\n",
      "Hair the looks that that desay you bether for your that\n",
      "Flettly Vushild teajm. Then you his pair wich arbows? I\n",
      "'Twn, as I hope: thy muster down, husdows commity\n",
      "sontmmetther scorrow she some thee,\n",
      "Shalts to fatattells been in make right: Ponstine,\n",
      "Whose grave in beast at crew contrut own maje,\n",
      "And then meranted and to my is our than that brew me wan mostor so provotious revise\n",
      "To hel but surre his in it, to hold too?\n",
      "\n",
      "JULIET:\n",
      "Ca'ens, this frape rome, what those an\n",
      "what us heart our out thy whip.\n",
      "\n",
      "WARWICHAM:\n",
      "My same? controw stind littes have the hurse!\n",
      "were broys sout on.\n",
      "\n",
      "BUCKINGHAMILLIO:\n",
      "Hence,\n",
      "Come these privest no mprive: t\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2920dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And I say yet, I wordds for in me as racchtanted I swern:\n",
      "Arwign.\n",
      "\n",
      "PRINCENTHUMERLET:\n",
      "Or will a\n",
      "Right.I\n",
      "\n",
      "HENMAM:\n",
      "Take I my fear tooth! you her: whence a made behom is as\n",
      "crutio's ded leets\n",
      "I show, these a lie twers, she\n",
      "we bodds that I smews it\n",
      "On knirgs pesenity and you.\n",
      "\n",
      "ROMEO:\n",
      "The scham, doth:\n",
      "Thouning you his in jue has he is by trams your crown,\n",
      "Wat?\n",
      "\n",
      "Sevill here fly measom to hath Is.\n",
      "Whilt say wilth how morme.\n",
      "And were is for mine in bunt you\n",
      "prity our sitter all the death an\n",
      "Kermil then\n",
      "aners this speent to Beceamintle;\n",
      "That belive ald of is the ind the what that\n",
      "Bexilansoless but oft as a lignors\n",
      "the fatts that errad-sinn, on.\n",
      "\n",
      "POF LAUDEENEN:\n",
      "Yet fathel time thies't! Petio, where spent's\n",
      "Remiant will then arge:\n",
      "But lie---dost tagge, that these hold, York,\n",
      "Of they serving are as huswele thy die?\n",
      "On thee pescied as matter nume,\n",
      "Marding full reascrow there in with greight: thou I\n",
      "wile day sood's duke's curnitions;\n",
      "Sheip agur our lookd wher seen while findsts manirs heir astor.\n",
      "\n",
      "MARIAS ILLANT:\n",
      "Halad up, rento me wisirit my posse dittread my these by anging.\n",
      "\n",
      "Firstnesd:\n",
      "Go woh myself good,\n",
      "No, whath let what to the watch more'st.\n",
      "What, here she tonge, of chultish rawd beling goinsted and\n",
      "Do makey to thinge ston Redure! smait; and privagtion.\n",
      "\n",
      "KING HORY VI:\n",
      "Fawhar were thee, lives, I quarn which either surs!\n",
      "And, balt body the likess on the masten,\n",
      "As get.\n",
      "\n",
      "ISABEMLLA:\n",
      "Who, arstom Winfove their broth\n",
      "the fropend Gileed us noble you holved?\n",
      "\n",
      "MPRONTES:\n",
      "Have in merding with I am old befixts?\n",
      "What warte what my grave too your hours.\n",
      "In jurpent yet love em Bandedier grones me.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Is will prince, give, Then suble the\n",
      "noly noe much hastingstand you-late\n",
      "worse with thee nobed will'd thy that his'\n",
      "Almen that mady their thon Mecrer as less of what'st speak up hears\n",
      "And be reap ail.\n",
      "\n",
      "BUGTHERSIO:\n",
      "Prook will slay trainst: by edly ritures lose no poor\n",
      "bray's life the was I\n",
      "Have their lettly wers, sirve-day the sout me,\n",
      "Whom I queeth broist stay, let to gentreike th\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b2fd5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLADYNENS:\n",
      "My spinief, you all not to fatwell,\n",
      "To mile dest on love we sity have eyben unstate may is all your have forth the wry\n",
      "To mest'pory would him were will his own they balindst we would\n",
      "\n",
      "And was mated my drestritt this, make an sweritars.\n",
      "\n",
      "PERDITH:\n",
      "Why, brother sweetom, frietle I then and my dour beWall ambreakth too to my blanish dlikes! Angainte moster and tought,\n",
      "And it town, once heir down, deep\n",
      "Our sleps it. yet be not Perial I\n",
      "But you the swift?\n",
      "Such sighams't an the efficy kingdom's death,\n",
      "Presst, words. Have curnest, But but to-me and rove.\n",
      "\n",
      "ROMEO:\n",
      "Hold make is hath in I wordse--\n",
      "Richard swien, wray: you, in and they be and:\n",
      "And greath you will King; what they low's love.\n",
      "Good while resperiom lifes a norther your glittless action.\n",
      "\n",
      "YORK:\n",
      "What seems you rew-man the beforn'd alongual on.\n",
      "My for Bauke thee, your his miriand prese in himst;\n",
      "And me somfult hears down the grow;\n",
      "Detrrow my were have me, till Frastiful Gented,\n",
      "And mannerle down is the lark; I wen you royal laDst you madie,\n",
      "Gry own their mine now, only grath,\n",
      "But breap dong dreep; the toimess.\n",
      "The tharte--drangul, wish our tongure in thee,\n",
      "Romans, you will the fly with ear love'st my not.\n",
      "\n",
      "QUEEN Stread's wife.\n",
      "\n",
      "MARWIARTER:\n",
      "Oun of all vanno:\n",
      "Natt nabire, that I\n",
      "\n",
      "Pentated will all are wind I fair take a simes.\n",
      "\n",
      "OXFORD:\n",
      "In that I wam to honoursel, look to hot.\n",
      "But as very numpled wouse word this;\n",
      "Or lidie; and by as they towen, py highersts father a sariught?\n",
      "\n",
      "PUS:\n",
      "Is it.\n",
      "What why, jeed with finding too the will knights\n",
      "To canse sub, And team; have come of men to gue to love.\n",
      "\n",
      "PENTES:\n",
      "Thed crientliman,\n",
      "You wind then is! fave whilips bey of the curtales met; I know,\n",
      "you ever lost that then depates us inquent,\n",
      "Bettake more of upon firess them answeary's evencty.\n",
      "Margair; as out, theshirds with forth\n",
      "And me speak what entrent-with\n",
      "May acpy. wiul--bentimy to the ead their Edwardning, my light,\n",
      "If untent that eyes, let lasch,-\n",
      "Which, boy my fortwer ply the with very the fattranIst of it.\n",
      "\n",
      "\n",
      "But NORHNS\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fefb8085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bake to the chuse:\n",
      "With Rome retunt it; vow there pity crowd\n",
      "My girt distreay a pote, ift styry timor for thee,\n",
      "But that the mintled to old up untely; unward's noblaus' rrvant;\n",
      "What so'dre will to me; is and then to hesper revile,\n",
      "Hethold then moot thy now be'd be our bed\n",
      "The have I now natw thou grair beavise with \n",
      "putcless armong them too, be dremance, what we she strast gives\n",
      "Which in wes ere his that is his priend you.\n",
      "\n",
      "HORTENSIO:\n",
      "What host your deaath, then, indun't,\n",
      "Hark on, that Haves difity me\n",
      "That Bauretering: re'er untanced my heart,\n",
      "Of a fathow cominstry here, God\n",
      "I have evoless you, thright not opor her, a consets or yeser your ure abd patch of us softent.\n",
      "\n",
      "PRINIUS:\n",
      "Way be yongely.\n",
      "O, the flaw as, Gnot\n",
      "that they mow abund, of an 'Tis sitse of him;\n",
      "This -marry some, these hande pasayoques'd moring\n",
      "this Somely my will toward to suful me then,\n",
      "those their Baputaya, ten; joy worth!\n",
      "\n",
      "GROMIOLANT:\n",
      "Gose and yours!\n",
      "Is honstrewl O Frince peace amilesd moly crumor wer Kursing bet's\n",
      "wising this dahnstight;-ne's entured\n",
      "And shonnoforte too dudese marded bold,\n",
      "Sweer is in is your but towne.\n",
      "\n",
      "ISABELLA:\n",
      "Yitter to you seve your words I had\n",
      "mantenters and that a my harte out on the slow and too the world-mainstry to mothin his fatiance the Desting is Till then dommant,\n",
      "And the lessing moarty, you may bew.\n",
      "\n",
      "WARWICK:\n",
      "I hold I cast in these that I have his with a casting,\n",
      "And boy then these not frace slinge\n",
      "Did, for Thet.\n",
      "\n",
      "QABELLA:\n",
      "As your brife; I am I seur, will, sity as broth, for as too say\n",
      "pritt, you wile thy lards\n",
      "Talth beot! and the quespost and diswring snew\n",
      "Ker to become the breast, then of brothming.\n",
      "\n",
      "Nurse:\n",
      "Go?\n",
      "What rether Fortunine's lout with\n",
      "His seform--twitill admed both again.\n",
      "; not your post knaltly at stable,\n",
      "The world wereing ensere in thou your bashm to,\n",
      "And foul that fly him. How whose then up yeep. Whith olds, buty I a sun\n",
      "all pract, I his arm old prothed\n",
      "Summs, appeed whom day him, and so to-thou wast though them reast,\n",
      "Gent them, you confed arm orign \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44581b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A! when these ever somrow word, that be love;\n",
      "And littles'd you reved, some of that dish but you that I\n",
      "renought?\n",
      "Let us sistread.\n",
      "Come; in no to\n",
      "Uposs I dispatter?\n",
      "\n",
      "CORIOLIOLA:\n",
      "Whost befouly these stilems, what thou hast exput. When high you that dote;\n",
      "Uncent, find he'rring a cure heaves.\n",
      "\n",
      "TRAMIO:\n",
      "By this cruch. You her, what, fints.\n",
      "\n",
      "KING RICHAND IIII:\n",
      "Rime, is not now braints:\n",
      "O.\n",
      "My nurse, I will seest by my dost?\n",
      "And she due up the lutt this; have shesse help mean your,\n",
      "Yet not in prace letts beforge to the from on;\n",
      "But lews moible it, let to the hourselfihts ture in yourt.\n",
      "\n",
      "BRUTUS:\n",
      "Firsts and plasity;\n",
      "Yem bunim that the alling thy shows Geenn beghn;\n",
      "My as my nown with up death thee the grove the wall not\n",
      "Those of is in head. I his ighmned be.\n",
      "\n",
      "MAMILLIUS:\n",
      "'Twer that maidstrum, Goot to you hohtright,\n",
      "And py to thos inder eving one,\n",
      "On your unnucham me? who doublerfess to Mornster.\n",
      "\n",
      "Sheedly have or four them, too but not doth'd; Doargut kne himselly speak\n",
      "Wartance will Clrittly in Wentuns\n",
      "And his princell\n",
      "A while defity Boly the not slait! God\n",
      "But in glast guar his engrave us worthal be\n",
      "Rule mermion the fattle how, seld are mistend we\n",
      "nows to right yourse be drought\n",
      "At all dong, the sleman, ement slawle,\n",
      "withates it me, the done, it, thou marials, that death munne,\n",
      "Sould will not provioleatings rightd at fittle the fortune,\n",
      "While shall by nust at afswer in beince of the men?\n",
      "Norn, bolient whalts not marrk your gone to most.\n",
      "\n",
      "BUCHOMIN:\n",
      "\n",
      "QUEEN ELIZABEDLIO:\n",
      "It the besidy I day to cautes exk, my I those sould for nargivile, this being bring bid.\n",
      "\n",
      "Seecuntan:\n",
      "I starded it but chall-on cry anstent: thou I straige admster to must of I know;\n",
      "Than' cot curt\n",
      "I die aggions, Gream was the domm.\n",
      "There hath mistys have armioned alter, huswer;\n",
      "AIn you can herer what uncly-pray,\n",
      "My slastanded shose and us\n",
      "ter of man, I my word\n",
      "To neers Of them my broth appuly so disperated of in in tillssed me up you thou grace,\n",
      "For bow this outher sloud: to my nielder apoy.\n",
      "\n",
      "PRINIUS:\n",
      "Is is Shenge\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37b2019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ROMEO:\n",
      "Yet not now.\n",
      "\n",
      "KING RICHARD II:\n",
      "At beash such, alt, if this woed my for Cliffity,\n",
      "Frier they oard\n",
      "With and this plickless I am per I will,\n",
      "Felle his it of my upleders?'s these crarity.\n",
      "\n",
      "KING RItHARD III:--though boy marre to would with your;\n",
      "For the sightly spwing you, entrely yourrs;\n",
      "Albeer'd it my grulings goodes in such the sign\n",
      "far Tertus\n",
      "of lost. HENENRY VI:\n",
      "Then here little, not I shall ressent that in have it.\n",
      "\n",
      "QUEEN ELBAHINA:\n",
      "O as will town be espurished\n",
      "Most, you, knows it passon if't'st the namdely then aluns of the mower pring;\n",
      "And, I'll for boarthleds me me.\n",
      "\n",
      "ANGEdway;\n",
      "And mile powe, it he.\n",
      "\n",
      "PAULINA:\n",
      "As were, both you mame morine.\n",
      "O ther, I and well.\n",
      "\n",
      "BRUTUS:\n",
      "Rigurantion, there to lard, your not a have drire of being they men.\n",
      "\n",
      "QUEEN ELIZABEY:\n",
      "Of reconsomy tae lws in Gentems\n",
      "It new, refore contrumm not born;\n",
      "If and it you cleibing syear these cittast,\n",
      "And while sove Cariole with whis lettle,\n",
      "For sman time put be, this twife.\n",
      "\n",
      "HORTENSIO:\n",
      "Touger all they strest,\n",
      "And I drinks dahber! whose thou fouget all say toity and somatuse\n",
      "Again I hone that his beeasom; as too fortweer if fly I longe with while.\n",
      "\n",
      "A\n",
      "POMPEY:\n",
      "Comince dose and fultiness, say you would.\n",
      "\n",
      "LUCIO:\n",
      "You but;\n",
      "And It knew shile and ontence\n",
      "becomia, a old octileming Hew\n",
      "Stive anth: you dother: at,\n",
      "For brieford, the his feed, they worth more dester;\n",
      "And by menter, an the goody been, the mouth;\n",
      "If all will the calling than with the dreasure thy weinn.\n",
      "Now crifend in hus my assung and find;\n",
      "And with this mence to breap tlemss for\n",
      "my at the pometer entlemost fortune pet,\n",
      "sincest be your vow make one thy ence,\n",
      "Nor consures brought both you unthan\n",
      "shamle?\n",
      "I with, swout no perpes. But now.\n",
      "Iffens youth he nabreat's dost their out.\n",
      "\n",
      "RICHARDORD:\n",
      "Well, father, is a of in stone, I was!\n",
      "Hat lost all singleing to death?\n",
      "What dread, dame you well'd earing:\n",
      "The bear their streath now his andone,\n",
      "For far this same, misiser cond the my breath.\n",
      "\n",
      "ROMEO:\n",
      "What this the mustrens and them preper and where sist.\n",
      "My\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a04c72cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Which a slong amming, all the grazenter you had to the namon.\n",
      "\n",
      "MONWIRD:\n",
      "Where's grace, their town, and their,\n",
      "So may my on my uncrient thurse, the seep with deam and leg that doth it,--loth was it that my the saw,\n",
      "And nemy burlowerly hearts Canning not, gives with doubbroth my true up\n",
      "And may my your hill were behosin these nal gauirsed afth.\n",
      "Vildonius go bleam lotgre he first fitt, seek end aluce his greast wordns but\n",
      "My paloud'd all chilent us heart futher you is steam I,\n",
      "That sweet us had not my won thy hath 'tis sweet.\n",
      "\n",
      "ROMEO:\n",
      "To prepide:\n",
      "Helesbress, I thou use wont, I and fill.\n",
      "\n",
      "COMINIUS:\n",
      "'Fortwer, doung, a wordring on emcund to his hence fatomfey's colving thee.\n",
      "\n",
      "KING Nove; for why noble\n",
      "alessing and what thon be. Second queen, I better,\n",
      "With Capue and kimmnal complest tradie his every king,\n",
      "While weer that our goodly me. You scair can is of\n",
      "donce wate.\n",
      "\n",
      "LORD:\n",
      "Go, you, they brove,\n",
      "To me bone am an betterry his loss,\n",
      "Trumst withip the blite?\n",
      "There menair case, I say are obell.\n",
      "\n",
      "CLARENCE:\n",
      "Vavinged, I ambst? My pecomes win'd:\n",
      "Father the banil; 'twer I botory thou\n",
      "Sicnre git. O, would us, as losts dies that your enceiz like.\n",
      "Yet not by they patce intun ender:\n",
      "Unclessings,\n",
      "As brin:---a\n",
      "\n",
      "You behating'd to fair'st, let Paful surre\n",
      "I hold nevingious no? and for my highd his his prinst conpe\n",
      "Viler the Ibellows my frogs, who news. is I wome ssub\n",
      "were to hearfenced what bear thy give,\n",
      "Than plase to be; and dukaws to-day\n",
      "Which is a nome and readdes.\n",
      "\n",
      "JOY GlOfencey, sir, Grewithy, no a me night'd tear's frives mistry dote.\n",
      "with-dentwry twile herfull or you exe.\n",
      "\n",
      "DUKE VINS Richios:\n",
      "Yet you no grow my pliect, what I would and not san measom\n",
      "Endavins make the knowy\n",
      "That I do vilives\n",
      "And we with name in bad; this bed.\n",
      "\n",
      "HORTENSAP:\n",
      "Cemell, o't town sheepary, and can why day\n",
      "And will tan thee woven in not to to forth rovost,\n",
      "His strought be thing onper these:\n",
      "Fear in my feam twere in with them, oncity entoo;\n",
      "Yiellit nourse menuse of the small'd his dight;\n",
      "And leavedow me that don\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:summer]",
   "language": "python",
   "name": "conda-env-summer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
